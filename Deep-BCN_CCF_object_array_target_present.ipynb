{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import tensorflow.contrib.slim as slim\n",
    "from nets.vgg import vgg_16 as vgg\n",
    "from nets.vgg import vgg_arg_scope\n",
    "import preprocessing.imagenet_utils as imagenet_utils\n",
    "import preprocessing.vgg as preprocessing\n",
    "import PIL.Image as Image\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "\n",
    "from MASC_core import MASC_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions \n",
    "\n",
    "def load_image(img_path):\n",
    "    print(\"Loading image\")\n",
    "    img = np.array(Image.open(img_path))\n",
    "    #img = imresize(img, (224, 224))\n",
    "    # Converting shape from [224,224,3] tp [1,224,224,3]\n",
    "    #x = np.expand_dims(img, axis=0)\n",
    "    # Converting RGB to BGR for VGG\n",
    "    #x = x[:,:,:,::-1]\n",
    "    return img\n",
    "\n",
    "    \n",
    "def image_crop(im, bbox):\n",
    "    return im[bbox[0]:bbox[0]+bbox[2], bbox[1]:bbox[1]+bbox[3],:]\n",
    "\n",
    "\n",
    "def gauss2D(shape,sigma):\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    return h\n",
    "\n",
    "\n",
    "# importing the model\n",
    "\n",
    "model_path = './modelparams/vgg_16.ckpt'\n",
    "labels_to_names, label_names = imagenet_utils.create_readable_names_for_imagenet_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters \n",
    "\n",
    "# RF Organization\n",
    "    \n",
    "crop_sizes= np.array([[200, 200], [200, 200], [250, 250], [250, 250], [300, 300], [350, 350], [400, 400], [450, 450], [500, 500]])\n",
    "\n",
    "row_steps = [100+100*x for x in range(5)] \n",
    "col_steps = [100+100*x for x in range(8)] \n",
    "row_shifts = np.concatenate((np.negative(row_steps[::-1]) , [0] , (row_steps)), axis=0) \n",
    "col_shifts = np.concatenate((np.negative(col_steps[::-1]) , [0] , (col_steps)), axis=0) \n",
    "\n",
    "# for filename in glob.glob('final_images/*.jpg'):\n",
    "#test_images_path = [glob.glob(test_images_dir + \"*.jpg\")][0]\n",
    "\n",
    "#image_name = 'used_TP/COCO_val2014_000000117899.jpg'\n",
    "\n",
    "#read in the search displays data files\n",
    "search_displays_data = pd.read_excel('object_array_displays_fixation_data/object_array_displays_data_present.xlsx')\n",
    "test_images_dir = \"object_array_displays_fixation_data/search_displays\"\n",
    "\n",
    "cates = ['clock' ,'crib','fan','garbage_can','microwave_oven','rug','socks','teddy_bear','tent']\n",
    "\n",
    "imagenet_cat_dict = {\n",
    "    'clock' : 409,\n",
    "    'crib'  : 520,\n",
    "    'fan'   : 545,\n",
    "    'garbage_can'    : 412,\n",
    "    'microwave_oven' : 651,\n",
    "    'rug'   : 741,\n",
    "    'socks' : 806,\n",
    "    'teddy_bear'     : 850,\n",
    "    'tent'  : 672 \n",
    "}\n",
    "\n",
    "max_num_fixs = 20\n",
    "use_image_foreground_for_object_arrays = 1\n",
    "\n",
    "use_GradCAM = 0\n",
    "use_CCF = 1\n",
    "if (use_CCF):\n",
    "    CCF_w = np.zeros( (len(cates),512))\n",
    "    for c in range(len(cates)):\n",
    "        image_dir = \"./object_array_displays_fixation_data/CCFs_object_array_categories/\"\n",
    "        w = np.load(image_dir + cates[c] + '_conv5_3_mean_map_w.npy')\n",
    "        CCF_w[c,:] = w / np.max(w)\n",
    "\n",
    "apply_IOR = 1\n",
    "IOR_size = 200  # pixels\n",
    "IOR_offset = math.floor(IOR_size/2)\n",
    "IOR_sigma = IOR_size/2 ;\n",
    "filt_IOR = gauss2D([IOR_size,IOR_size], IOR_sigma) \n",
    "filt_IOR = filt_IOR/np.max(filt_IOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "all_fixations = np.zeros((len(search_displays_data),max_num_fixs,2))\n",
    "target_presence = np.zeros((len(search_displays_data)))\n",
    "\n",
    "for im_num in range(len(search_displays_data)):\n",
    "    \n",
    "    \n",
    "    cat_name = search_displays_data[\"categorical_cue\"][im_num]\n",
    "    cat_name = '_'.join(cat_name.split())\n",
    "    \n",
    "    search_target = imagenet_cat_dict[cat_name] \n",
    "    print(cat_name + '    ' + str(search_target) )\n",
    "    image_name = search_displays_data[\"disp_im_name\"][im_num]\n",
    "    \n",
    "    image_path = test_images_dir + '/' + image_name\n",
    "    print(image_path)\n",
    "\n",
    "    search_im = load_image(image_path)\n",
    "    \n",
    "    with open('./Image_names_object_array_dislays.txt', 'a') as f1:\n",
    "        f1.write(image_name + os.linesep)\n",
    "    \n",
    "    plt.imshow(search_im)\n",
    "    plt.show()\n",
    "    print(search_im.shape)\n",
    "    im_size = search_im.shape\n",
    "    im_h = im_size[0]\n",
    "    im_w = im_size[1]\n",
    "\n",
    "    fixation_map = np.zeros((im_h+IOR_size,im_w+IOR_size));\n",
    "\n",
    "    im_col_fix = math.floor(im_w/2) # 493 #\n",
    "    im_row_fix = math.floor(im_h/2)  # 673 #\n",
    "    \n",
    "    im_corner_row = 1500-math.floor(im_h/2)\n",
    "    im_corner_col = 2000-math.floor(im_w/2) \n",
    "\n",
    "    fixations_row = np.zeros((max_num_fixs))\n",
    "    fixations_col = np.zeros((max_num_fixs))\n",
    "    \n",
    "    fixations_row[0] = im_row_fix\n",
    "    fixations_col[0] = im_col_fix\n",
    "\n",
    "    for fix in range(1,max_num_fixs):\n",
    "        \n",
    "        print(\"Eye_movement  \" + str(fix))\n",
    "\n",
    "        row_fix = im_corner_row + im_row_fix \n",
    "        col_fix = im_corner_col + im_col_fix\n",
    "\n",
    "        im_foreground = 255-np.mean(search_im.astype(float), axis=2)\n",
    "        im_foreground_norm = im_foreground #np.multiply(im_foreground,np.divide(im_foreground, im_foreground))\n",
    "        nonzero = im_foreground > 0\n",
    "        im_foreground_norm[nonzero] /= im_foreground[nonzero]\n",
    "    #     plt.imshow(im_foreground_norm)\n",
    "    #     plt.show()\n",
    "\n",
    "        backg = 255*np.ones((3000,4000,3))\n",
    "        backg[im_corner_row:im_corner_row + im_h,im_corner_col:im_corner_col+im_w,:]=search_im;\n",
    "        backg = np.uint8(backg)\n",
    "#         plt.imshow(backg)\n",
    "#         plt.show()\n",
    "\n",
    "        backg_RF = backg;\n",
    "        sample_size = len(col_shifts)* len(row_shifts) \n",
    "        conv5_3_feat = np.zeros((sample_size,14,14,512),dtype=float)\n",
    "        #conv5_3_grads = np.zeros((sample_size,14,14,512),dtype=float)\n",
    "        conv5_3_p = np.zeros((sample_size,14,14),dtype=float)\n",
    "\n",
    "        conv5_3_p_ub = np.zeros((sample_size,14,14),dtype=float)\n",
    "\n",
    "        crop_box_center_all = np.zeros((sample_size,2),dtype=float)\n",
    "        crop_box_corner_all = np.zeros((sample_size,2),dtype=float)\n",
    "\n",
    "        g_2 = tf.Graph()          \n",
    "        with g_2.as_default():\n",
    "            vgg_inputs = tf.placeholder(dtype=tf.float32, shape=[None, 224, 224, 3], name='input_image')\n",
    "            with slim.arg_scope(vgg_arg_scope()):\n",
    "                vgg_output, vgg_endpoints = vgg(vgg_inputs, is_training=False)\n",
    "\n",
    "            # todo: Compute gradient layer by layer\n",
    "            init_fn = slim.assign_from_checkpoint_fn(model_path, slim.get_model_variables('vgg_16'))\n",
    "\n",
    "            sess_2 = tf.Session(graph=g_2)\n",
    "            sess_2.run(tf.global_variables_initializer())\n",
    "            init_fn(sess_2)\n",
    "            crop_index_overall = 0\n",
    "            for row_RF in range(len(row_shifts)):  # \n",
    "\n",
    "                sample_size = len(col_shifts)\n",
    "                crop_set = np.zeros((sample_size,224,224,3),dtype=float)\n",
    "                crop_index = 0\n",
    "\n",
    "                for col_RF in range(len(col_shifts)):  #\n",
    "                    row_dis = abs(math.floor((len(row_shifts)/2)) - row_RF) \n",
    "                    col_dis = abs(math.floor((len(col_shifts)/2)) - col_RF) \n",
    "                    crop_ind = max(row_dis,col_dis)\n",
    "\n",
    "                    crop_size = crop_sizes[crop_ind]\n",
    "                    #print(crop_size)\n",
    "                    crop_box_center = np.array([row_fix + row_shifts[row_RF] , col_fix + col_shifts[col_RF]]) \n",
    "                    crop_box_center_all[crop_index_overall] = crop_box_center\n",
    "                    #print(crop_box_center)\n",
    "                    crop_corner = crop_box_center - math.floor(crop_size[0]/2) \n",
    "                    crop_box_corner_all[crop_index_overall] = crop_corner\n",
    "                    #print(crop_corner)\n",
    "                    im_crop = image_crop(backg_RF,np.concatenate((crop_corner, crop_size), axis = 0)) \n",
    "\n",
    "                    s_image = Image.fromarray(im_crop)\n",
    "                    s_image = s_image.resize([224, 224], resample=Image.BICUBIC)\n",
    "                    im_crop_re = np.asarray(s_image, dtype=np.float32)\n",
    "\n",
    "                    if row_RF == math.floor(len(row_shifts)/2) and col_RF == math.floor(len(col_shifts)/2):  \n",
    "                        plt.imshow(s_image)\n",
    "                        plt.show()\n",
    "                        time.sleep(0.5)\n",
    "\n",
    "            #         if len(s_image.shape) < 3:\n",
    "            #             print(\"Image is not RGB 3Dimensional data\")\n",
    "                    im_crop_re = preprocessing.mean_image_subtraction(im_crop_re)\n",
    "                    im_crop_re = np.expand_dims(im_crop_re, axis=0)\n",
    "\n",
    "                    crop_set[crop_index,:,:,:] = im_crop_re\n",
    "                    crop_index = crop_index + 1 \n",
    "                    crop_index_overall = crop_index_overall + 1\n",
    "                  # crop_ind = crop_array(col_RF);\n",
    "\n",
    "                #print('Running the session')\n",
    "                vgg_all_output = sess_2.run([vgg_output, vgg_endpoints], feed_dict={vgg_inputs: crop_set})\n",
    "                [current_vgg_output, current_vgg_endpoints] = vgg_all_output\n",
    "                \n",
    "                conv5_3_f = current_vgg_endpoints['vgg_16/conv5/conv5_3']                \n",
    "\n",
    "                if row_RF == math.floor(len(row_shifts)/2):\n",
    "                    \n",
    "                    print([\"Fixated area of the image and the classificatoin results\"])\n",
    "\n",
    "                    s_output = current_vgg_output[math.floor(len(col_shifts)/2)]\n",
    "                    sorted_cates = np.argsort(-s_output)\n",
    "                    selected_cates = sorted_cates[0:5]\n",
    "\n",
    "                    for i, catId in enumerate(selected_cates):\n",
    "                        # !!!: Notice here this is a catId+1 instead of catID\n",
    "                        print('{:d}\\t category: {:s}; confidence:{:.4f},\\tlabelID: {:s} {:d}'.format(i, labels_to_names[\n",
    "                            catId + 1], s_output[catId], label_names[catId], catId))\n",
    "\n",
    "                if use_GradCAM:\n",
    "                # apply grad-cam\n",
    "\n",
    "                    layer_name =  'vgg_16/conv5/conv5_3'\n",
    "                    conv_layer = vgg_endpoints[layer_name]  #'vgg_16/conv5/conv5_3'\n",
    "\n",
    "                    one_hot = tf.sparse_to_dense(search_target, [1000], 1.0)\n",
    "                    signal_one_hot = tf.multiply(vgg_endpoints['vgg_16/fc8'], one_hot)\n",
    "                    loss = tf.reduce_mean(signal_one_hot)\n",
    "\n",
    "                    grads = tf.gradients(loss, conv_layer)[0]\n",
    "                #     # Normalizing the gradients\n",
    "                    norm_grads = tf.div(grads, tf.sqrt(tf.reduce_mean(tf.square(grads))) + tf.constant(1e-5))\n",
    "\n",
    "                    outputs, grads_val, loss_val = sess_2.run([conv_layer, norm_grads, loss], feed_dict={vgg_inputs: crop_set})\n",
    "            #         output = output[0]           # [7,7,512]\n",
    "            #         grads_val = grads_val[0]\t # [7,7,512]\n",
    "                    outputs = np.asarray(outputs)\n",
    "                    grads_val = np.asarray(grads_val)\n",
    "                    #outputs_re = np.reshape(outputs,(outputs.shape[0]*outputs.shape[1]*outputs.shape[2], outputs.shape[3]))\n",
    "\n",
    "                    all_weights = np.mean(grads_val, axis = (1, 2)) \t\t\t# [?,512]\n",
    "                    all_weights = all_weights/np.max(all_weights)\n",
    "                    p = np.ones(outputs.shape[0 : 3], dtype = np.float32)\t# [?,14,14]\n",
    "                    p_ub = np.ones(outputs.shape[0 : 3], dtype=np.float32)  # [?,14,14]    \n",
    "                    #np.matmul(output_re, np.transpose(weights))\n",
    "\n",
    "                    for j in range(outputs.shape[0]):\n",
    "                        weights = all_weights[j,:]\n",
    "                        output = outputs[j,:]\n",
    "            #           # Taking a weighted average\n",
    "                        for i, w in enumerate(weights):\n",
    "                            p[j,:] += w * output[:, :, i]\n",
    "                        \n",
    "                        \n",
    "                elif(use_CCF == 1):\n",
    "                                                     \n",
    "                    weights = CCF_w[cates.index(cat_name),:]\n",
    "                    \n",
    "                    p = np.matmul(conv5_3_f,weights)\n",
    "                        \n",
    "                \n",
    "                p_ub = np.mean(conv5_3_f, axis=3 )\n",
    "\n",
    "                ind_beg=17*(row_RF)\n",
    "                conv5_3_feat[ind_beg:ind_beg+17,:,:,:] = current_vgg_endpoints['vgg_16/conv5/conv5_3']\n",
    "                #conv5_3_grads[ind_beg:ind_beg+17,:,:,:] = grads_val\n",
    "                conv5_3_p[ind_beg:ind_beg+17,:,:] = p\n",
    "\n",
    "                conv5_3_p_ub[ind_beg:ind_beg+17,:,:] = p_ub\n",
    "\n",
    "\n",
    "        if(s_output[search_target] > 5  and search_target in selected_cates[:5]):\n",
    "            target_presence[im_num] = 1\n",
    "            break\n",
    "        \n",
    "        priority_big = np.zeros((3000,4000))\n",
    "\n",
    "        crop_box_center_all = crop_box_center_all.astype(int)\n",
    "        priority_big[crop_box_center_all[:,0], crop_box_center_all[:,1]] = np.max(conv5_3_p, axis = (1, 2))\n",
    "\n",
    "        filt = gauss2D( (200,200), 100)\n",
    "        priority_big_map = signal.fftconvolve(priority_big, filt, mode='same')\n",
    "\n",
    "        priority_map = priority_big_map[im_corner_row:im_corner_row+im_h, im_corner_col:im_corner_col + im_w]\n",
    "        priority_map = priority_map/np.max(priority_map)\n",
    "        #priority_map = priority_map \n",
    "        if (use_image_foreground_for_object_arrays == 1):\n",
    "            priority_map = np.multiply(im_foreground_norm, priority_map)\n",
    "\n",
    "        if (apply_IOR): \n",
    "            #fixation_map[row_im_f+IOR_offset,col_im_f+IOR_offset] = 1 \n",
    "            fixation_map[im_row_fix:im_row_fix+2*IOR_offset,im_col_fix:im_col_fix+2*IOR_offset] = \\\n",
    "                fixation_map[im_row_fix:im_row_fix+2*IOR_offset,im_col_fix:im_col_fix+2*IOR_offset] + filt_IOR\n",
    "            IOR_map = fixation_map[IOR_offset:IOR_offset+im_h,IOR_offset:IOR_offset+im_w] \n",
    "            #IOR_map = IOR_map / np.max(IOR_map)\n",
    "            priority_map_fixs = priority_map - IOR_map \n",
    "            priority_map_fixs= priority_map_fixs.clip(min=0)\n",
    "            \n",
    "        print(\"Priority map\")\n",
    "        plt.imshow(priority_map_fixs, cmap='gray')  # _fixs/np.max(priority_map_fixs)\n",
    "        plt.show()\n",
    "        time.sleep(0.5)\n",
    "        print(np.max(priority_map_fixs))\n",
    "        if(np.max(priority_map_fixs) < 0.4):\n",
    "            break\n",
    "            \n",
    "        # MASC_core is the main function \n",
    "        # input arguments: \n",
    "        #     priority_map: Priority map\n",
    "        #     RETINA_PIXDEG: The number of pixels in one degree visual angle of the visual display  \n",
    "        #     im_col_fix: The column for the current fixation (x)\n",
    "        #     im_row_fix: The row for the current fixtion (y)\n",
    "        # outputs: \n",
    "        #     col_im_m: The column for the next fixation (x)\n",
    "        #     row_im_m: The row for the next fixation (x)\n",
    "        #     moto_Coll_framed: The activity map in the motor layer of the SC \n",
    "        #     col_m_coll: the column coordiatne of the winning population in the SC motor map\n",
    "        #     row_m_coll: the row coordiatne of the winning population in the SC motor map\n",
    "\n",
    "        RETINA_PIXDEG = 22\n",
    "        col_im_m, row_im_m, moto_Coll_framed, col_m_coll, row_m_coll = MASC_core(priority_map_fixs, RETINA_PIXDEG, im_col_fix, im_row_fix)\n",
    "\n",
    "\n",
    "\n",
    "        plt.imshow(search_im, aspect='auto')\n",
    "        plt.plot([im_col_fix, col_im_m], [im_row_fix, row_im_m], '-o', color='lawngreen')\n",
    "        plt.plot(col_im_m, row_im_m, 'o',color='red')\n",
    "        plt.show()\n",
    "\n",
    "        plt.imshow(moto_Coll_framed, aspect='auto')\n",
    "        plt.plot(col_m_coll, row_m_coll, 'o',color='red')\n",
    "        plt.show()\n",
    "\n",
    "        im_col_fix = col_im_m\n",
    "        im_row_fix = row_im_m\n",
    "\n",
    "        fixations_row[fix] = im_row_fix\n",
    "        fixations_col[fix] = im_col_fix\n",
    "\n",
    "    plt.imshow(search_im, aspect='auto')\n",
    "    plt.plot(fixations_col[0:fix], fixations_row[0:fix], '-o', color='lawngreen')\n",
    "    plt.plot(fixations_col[fix-1], fixations_row[fix-1], 'o',color='red')\n",
    "    plt.savefig('./object_array_displays_fixation_data/model_scanpaths_present/' + image_name.split('.')[0] + \\\n",
    "                '_scanpath_'+ str(target_presence[im_num]) + '.jpg')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    all_fixations[im_num,:,0] = fixations_col\n",
    "    all_fixations[im_num,:,1] = fixations_row\n",
    "    \n",
    "    np.save('Deep-BCN_CCF_object_array_target_present.npy',all_fixations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
